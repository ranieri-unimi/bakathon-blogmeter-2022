---
title: "Challenge 1 - Feature Engineering"
author: "Marco Zanotti"
date: "2021-2022"
output: 
  rmdformats::downcute:
    highlight: tango
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(timetk)
```

## Content

This challenge is all about explorative data analysis (EDA) and
features engineering in time series (Lectures 1 and 2).  

The purpose of this challenge is to test your ability to effectively understand 
the dynamics of different time series allowing you to create one or more 
features' recipe.  

The analysis you perform at this first stage will be the basis to perform accurate
forecasts during the next challenges.  



## Requirements

You are required to produce a fully reproducible notebook report explaining 
all the steps that takes you towards the creation of all the new features 
you would like to use in forecasting models.  

At least one feature set for each time series has to be produced.  



## Data

Challenge dataset contains:  
- daily website page views   
- hourly website organic searches  
- weekly website sales  

```{r data, echo=FALSE}
data = read_rds("../data/challenges_dataset.rds")
data %>% 
  map(~ select(., 1:2)) %>% 
  map(~ pivot_longer(., -date, names_to = "series")) %>% 
  bind_rows() %>% 
  plot_time_series(.date_var = date, .value = value, .facet_vars = series, .facet_scales = "free")
```


```{r}
data = read_rds("data/challenges_dataset.rds")
```


```{r}
# to daily
subscribers_daily_tbl = subscribers_tbl %>%
  summarise_by_time(optin_time, .by = "day", optins = n())

subscribers_tbl %>%
  group_by(member_rating) %>%
  summarise_by_time(optin_time, .by = "day", optins = n()) %>%
  pivot_wider(names_from = member_rating, values_from = optins)
```

```{r}
# sum() for each day
analytics_daily_tbl = analytics_tbl %>%
  mutate(date = ymd_h(dateHour), .before = everything()) %>%
  select(-dateHour) %>%
  summarise_by_time(date, .by = "day", across(pageViews:sessions, .fns = sum))
```

```{r}
# to weekly
subscribers_tbl %>%
  summarize_by_time(optin_time, .by = "week", optins = n())
```

```{r}
# to monthly
subscribers_tbl %>%
  summarize_by_time(optin_time, .by = "month", optins = n())

```

```{r}
# * Pad by Time -----------------------------------------------------------
```

```{r}
# - Filling in time series gaps
# - Low-to-High Frequency (un-aggregating)
```

```{r}
# fill daily gaps, with zeroes???? (I'd rather use the avg u.u ) to replace NA values 
subscribers_daily_tbl %>%
  pad_by_time(.date_var = optin_time, .by = "day", .pad_value = 0, .start_date = "2018-06-01")
```

```{r}
# weekly to daily
subscribers_daily_tbl %>%
  pad_by_time(optin_time, .by = "day", .start_date = "2018-06") %>%
  mutate_by_time(.by = "week", optins = sum(optins, na.rm = TRUE) / 7)

```

```{r}
# * Filter by Time --------------------------------------------------------
```

```{r}
# - Pare data down before modeling

subscribers_daily_tbl %>%
  filter_by_time(.start_date = "2018-11-20")

subscribers_daily_tbl %>%
  filter_by_time(.start_date = "2019-12", .end_date = "2019")

subscribers_daily_tbl %>%
  filter_by_time(.start_date = "2019-12", .end_date = "2019-12-01" %+time% "4 weeks")

```

```{r}
# * Mutate by Time --------------------------------------------------------
```

```{r}
# - Get change from beginning/end of period
```

```{r}
# first, last, mean, median by period
# insteado of taking start-end diff, we'll look to summarize window-span

subscribers_daily_tbl %>%
  mutate_by_time(
    .by = "1 week",
    optins_mean = mean(optins),
    optins_median = median(optins),
    optins_max = max(optins),
    optins_min = min(optins)
  )

```

```{r}
# * Join by Time ----------------------------------------------------------
```

```{r}
# - Investigating Relationships
# - Identify External Regressors

subscribers_google_joined_tbl = subscribers_daily_tbl %>%
  pad_by_time(.pad_value = 0, .start_date = "2018-06") %>%
  left_join(analytics_daily_tbl, by = c("optin_time" = "date"))
```

```{r}
# inspect join
subscribers_google_joined_tbl %>% plot_missing() # kawaii
subscribers_google_joined_tbl %>% tk_summary_diagnostics() 
```

```{r}
# plot relationships
subscribers_google_joined_tbl %>%
  pivot_longer(-optin_time) %>%
  plot_time_series(optin_time, value, .color_var = name, .smooth = FALSE)

subscribers_google_joined_tbl %>%
  drop_na() %>%
  mutate(across(optins:sessions, .fns = log1p)) %>%
  mutate(across(optins:sessions, .fns = standardize_vec)) %>%
  pivot_longer(optins:sessions) %>%
  plot_time_series(optin_time, value, name, .smooth = FALSE)

```

```{r}
# * Future Frame ----------------------------------------------------------
```

```{r}
# - Forecasting helper

subscribers_day_tbl = subscribers_tbl %>%
  summarise_by_time(.date_var = optin_time, .by = "day", optins = n()) %>%
  pad_by_time(.by = "day", .pad_value = 0)

subscribers_daily_tbl %>%
  future_frame(.length_out = 10)
```

```{r}
# modelling example
model_fit_lm = lm(
  optins ~ as.numeric(optin_time) + wday(optin_time, label = TRUE),
  data = subscribers_daily_tbl
)

summary(model_fit_lm) # look at the sky, its a bird its a plane


future_tbl = subscribers_daily_tbl %>%
  future_frame(.length_out = "2 months")

predictions_vec = predict(model_fit_lm, newdata = future_tbl) %>% as.vector() # forcaster

subscribers_daily_tbl %>%
  select(optin_time, optins) %>%
  add_column(type = "actual") %>%
  bind_rows(
    future_tbl %>%
      mutate(optins = predictions_vec, type = "prediction")
  ) %>%
  plot_time_series(optin_time, optins, type, .smooth = FALSE)


```

```{r}
# Transformation ----------------------------------------------------------

subscribers_daily_tbl = subscribers_tbl %>%
  summarise_by_time(.date_var = optin_time, .by = "day", optins = n()) %>%
  pad_by_time(.date_var = optin_time, .by = "day", .pad_value = 0)
```

```{r}
# * Variance Reduction ----------------------------------------------------
```

```{r}
# Log = never
subscribers_daily_tbl %>%
  mutate(optins = log(optins))
```

```{r}
# Log + 1 = always*
subscribers_daily_tbl %>%
  mutate(optins = log1p(optins)) %>%
  plot_time_series(optin_time, optins)
```

```{r}
# - inversion with exp() and expm1()
```

```{r}
# Box-Cox
subscribers_daily_tbl %>%
  mutate(optins = box_cox_vec(optins + 1, lambda = "auto")) %>%
  plot_time_series(optin_time, optins)
```

```{r}
# - inversion with box_cox_inv_vec()

```

```{r}
# * Range Reduction -------------------------------------------------------
```

```{r}
# - Used in visualization to overlay series
# - Used in ML for models that are affected by feature magnitude (e.g. linear regression)
```

```{r}
# Normalization Range (0,1) = MAX-MIN (maybe logit uwu)
analytics_daily_tbl %>%
  pivot_longer(-date) %>%
  group_by(name) %>%
  mutate(value = normalize_vec(value)) %>%
  plot_time_series(date, value)
```

```{r}
# Standardization
analytics_daily_tbl %>%
  pivot_longer(-date) %>%
  group_by(name) %>%
  mutate(value = standardize_vec(value)) %>%
  plot_time_series(date, value)

```

```{r}
# * Smoothing -------------------------------------------------------------
 # = iacone said that's nasty, dont do it
```

```{r}
# - Identify trends and cycles
# - Clean seasonality

subscribers_daily_tbl %>%
  mutate(
    optins = log1p(optins),
    optins_smooth = smooth_vec(optins, period = 24 * 7, degree = 0)
  ) %>%
  pivot_longer(-optin_time) %>%
  plot_time_series(optin_time, value, .color_var = name, .smooth = FALSE)

analytics_daily_tbl %>%
  pivot_longer(-date) %>%
  group_by(name) %>%
  mutate(value_smooth = smooth_vec(value, period = 24 * 7, degree = 0)) %>%
  pivot_longer(contains("value"), names_repair = "unique") %>%
  rename(name = `name...2`, type = `name...3`) %>%
  group_by(name) %>%
  plot_time_series(date, value, .color_var = type, .smooth = FALSE)

```

```{r}
# * Rolling Averages ------------------------------------------------------
```

```{r}
# - Common time series operations to visualize trend
# - A simple transformation that can help create improve features
# - Can help with outlier-effect reduction & trend detection
# - Note: Businesses often use a rolling average as a forecasting technique
# A rolling average forecast is usually sub-optimal (good opportunity for you!).

analytics_daily_tbl %>%
  pivot_longer(-date) %>%
  group_by(name) %>%
  mutate(
    value_roll = slidify_vec(
      value,
      .f = mean,
      .period = 24 * 7, # 6 months = 7 days * 24 weeks
      .align = "center",
      .partial = TRUE
    )
  ) %>%
  pivot_longer(contains("value"), names_repair = "unique") %>%
  rename(name = `name...2`, type = `name...3`) %>%
  group_by(name) %>%
  plot_time_series(date, value, .color_var = type, .smooth = FALSE)

```

```{r}
# * Missing Values Imputation ---------------------------------------------
```

```{r}
# - Imputation helps with filling gaps (if needed)

subscribers_daily_tbl %>%
  mutate(optins_na = ifelse(optins == 0, NA, optins)) %>%
  mutate(optins_imputed = ts_impute_vec(optins_na, period = 7)) %>%
  pivot_longer(-optin_time) %>%
  plot_time_series(optin_time, log1p(value), .color_var = name, .smooth = FALSE)

  # fill with avg, but spicy (a bit of MA)

```

```{r}
# * Anomaly Cleaning ------------------------------------------------------
```

```{r}
# - Outlier removal helps linear regression detect trend and reduces high leverage points
# WARNING: Make sure you check outliers against events
# - usually there is a reason for large values
```

```{r}
# Anomaly detection
subscribers_daily_tbl %>%
  plot_anomaly_diagnostics(optin_time, optins)

subscribers_daily_tbl %>%
  plot_anomaly_diagnostics(optin_time, log1p(optins))

subscribers_cleaned_daily_tbl = subscribers_daily_tbl %>%
  mutate(
    optins_log = log1p(optins),
    optins_cleaned = ts_clean_vec(optins, period = 7),
    optins_log_cleaned = ts_clean_vec(optins_log, period = 7)
  )

subscribers_cleaned_daily_tbl %>%
  pivot_longer(-optin_time) %>%
  mutate(
    cleaned = ifelse(str_detect(name, "cleaned"), "cleaned", "level"),
    type = ifelse(str_detect(name, "log"), "log", "level")
  ) %>%
  plot_time_series(optin_time, value, cleaned, .facet_vars = type, .smooth = FALSE)
```

```{r}
# without log
# outlier effect - before cleaning
subscribers_cleaned_daily_tbl %>%
  plot_time_series_regression(
    optin_time,
    .formula = optins ~ as.numeric(optin_time) +
      wday(optin_time, label = TRUE) +
      month(optin_time, label = TRUE),
    .show_summary = TRUE
  )
```

```{r}
# outlier effect - after cleaning
subscribers_cleaned_daily_tbl %>%
  plot_time_series_regression(
    optin_time,
    .formula = optins_cleaned ~ as.numeric(optin_time) +
      wday(optin_time, label = TRUE) +
      month(optin_time, label = TRUE),
    .show_summary = TRUE
  )
```

```{r}
# with log
# outlier effect - before cleaning
subscribers_cleaned_daily_tbl %>%
  plot_time_series_regression(
    optin_time,
    .formula = optins_log ~ as.numeric(optin_time) +
      wday(optin_time, label = TRUE) +
      month(optin_time, label = TRUE),
    .show_summary = TRUE
  )
```

```{r}
# outlier effect - after cleaning
subscribers_cleaned_daily_tbl %>%
  plot_time_series_regression(
    optin_time,
    .formula = optins_log_cleaned ~ as.numeric(optin_time) +
      wday(optin_time, label = TRUE) +
      month(optin_time, label = TRUE),
    .show_summary = TRUE
  )

```

```{r}
# * Lags & Differencing ---------------------------------------------------
```

```{r}
# - Lags: Often used for feature engineering
# - Lags: Autocorrelation
# - MOST IMPORTANT: Can possibly use lagged variables in a model, if lags are correlated
# - Difference: Used to go from growth to change
# - Difference: Makes a series "stationary" (potentially)
```

```{r}
# lags
subscribers_daily_tbl %>%
  mutate(optins_lag_1 = lag_vec(optins, lag = 1))

subscribers_daily_tbl %>%
  tk_augment_lags(.value = optins, .lags = c(1, 2, 7, 14))
```

```{r}
# differencing
analytics_daily_tbl %>%
  mutate(across(pageViews:sessions, .fns = diff_vec)) %>%
  pivot_longer(-date) %>%
  plot_time_series(date, value, name, .smooth = FALSE)

```

```{r}
# * Fourier Transform ------------------------------------------------------
```

```{r}
# - Useful for incorporating seasonality & autocorrelation
# - BENEFIT: Don't need a lag, just need a frequency (based on your time index)
```

```{r}
# single fourier series
subscribers_daily_tbl %>%
  mutate(sin14_k1 = fourier_vec(optin_time, period = 14, K = 1, type = "sin")) %>%
  mutate(cos14_k1 = fourier_vec(optin_time, period = 14, K = 1, type = "cos")) %>%
  select(-optins) %>%
  pivot_longer(matches("(cos)|(sin)")) %>%
  plot_time_series(optin_time, value, name, .smooth = FALSE)
```

```{r}
# multiple fourier series
subscribers_daily_tbl %>%
  tk_augment_fourier(optin_time, .periods = c(14, 30, 90, 365), .K = 2) %>%
  plot_time_series_regression(
    optin_time,
    .formula = log1p(optins) ~ as.numeric(optin_time) + . - optin_time,
    .show_summary = TRUE
  )

```

```{r}
# * Confined Interval -----------------------------------------------------
```

```{r}
# - Transformation used to confine forecasts to a max/min interval

subscribers_daily_tbl %>%
  plot_time_series(
    optin_time,
    log_interval_vec(optins, limit_lower = 0, offset = 1)
  )


```

```{r}
# Visualization -----------------------------------------------------------

subscribers_day_tbl = subscribers_tbl %>%
  summarise_by_time(.date_var = optin_time, .by = "day", optins = n()) %>%
  pad_by_time(.by = "day", .pad_value = 0)

analytics_long_hour_tbl = analytics_tbl %>%
  mutate(date = ymd_h(dateHour), .before = everything()) %>%
  select(-dateHour) %>%
  pivot_longer(cols = pageViews:sessions)

```

```{r}
# * Time Series Plot ------------------------------------------------------

subscribers_day_tbl %>%
  plot_time_series(optin_time, optins, .smooth = FALSE)

analytics_long_hour_tbl %>%
  plot_time_series(date, value, .color_var = name, .smooth = FALSE)

analytics_long_hour_tbl %>%
  plot_time_series(date, value, .color_var = name, .facet_vars = name, .smooth = FALSE)
```

```{r}
# Log Transforms
subscribers_day_tbl %>%
  plot_time_series(optin_time, log(optins + 1))

analytics_long_hour_tbl %>%
  plot_time_series(date, log(value + 1), .color_var = name, .facet_vars = name)

```

```{r}
# * Autocorrelation Function (ACF) Plot -----------------------------------

subscribers_day_tbl %>%
  plot_acf_diagnostics(optin_time, log(optins + 1), .lags = 10, .show_white_noise_bars = TRUE)

subscribers_day_tbl %>%
  plot_acf_diagnostics(optin_time, log(optins + 1), .lags = 500, .show_white_noise_bars = TRUE)

```

```{r}
# * Cross-Correlation Function (CCF) Plot ---------------------------------

subscribers_ga_day_tbl = subscribers_day_tbl %>%
  left_join(
    analytics_long_hour_tbl %>%
      pivot_wider(names_from = name, values_from = value) %>%
      summarise_by_time(.date_var = date,
                        .by = "day",
                        across(pageViews:sessions, .fns = sum)),
    by = c("optin_time" = "date")
  )

subscribers_ga_day_tbl %>%
  drop_na() %>%
  plot_acf_diagnostics(
    optin_time,
    optins,
    .ccf_vars = pageViews:sessions,
    .lags = 100,
    .show_white_noise_bars = TRUE,
    .show_ccf_vars_only = TRUE,
    .facet_ncol = 3,
  )

```

```{r}
# * Smoothing Plot --------------------------------------------------------

subscribers_day_tbl %>%
  plot_time_series(
    optin_time,
    log(optins + 1),
    .smooth_period = "90 days",
    .smooth_degree = 1
  )

```

```{r}
# * Seasonality Plot ------------------------------------------------------

subscribers_day_tbl  %>%
  plot_seasonal_diagnostics(optin_time, log(optins + 1))

```

```{r}
# * Decomposition Plot ----------------------------------------------------

subscribers_day_tbl %>%
  plot_stl_diagnostics(optin_time, log(optins + 1))

```

```{r}
# * Anomaly Detection Plot ------------------------------------------------

subscribers_day_tbl %>%
  tk_anomaly_diagnostics(optin_time, optins, .alpha = .01, .max_anomalies = .01)

subscribers_day_tbl %>%
  plot_anomaly_diagnostics(optin_time, optins, .alpha = .01, .max_anomalies = .01)

```

```{r}
# * Time Series Regression Plot -------------------------------------------

subscribers_day_tbl %>%
  plot_time_series_regression(
    optin_time,
    log(optins + 1) ~
      as.numeric(optin_time) + # linear trend
      wday(optin_time, label = TRUE) + # week day calendar features
      month(optin_time, label = TRUE), # month calendar features
    .show_summary = TRUE
  )
```